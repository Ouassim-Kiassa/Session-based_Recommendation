{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eb1c767",
   "metadata": {},
   "source": [
    "# RecSys Project - Group 33\n",
    "\n",
    "In this part of the project, we would like to try to implement a recommendation model form __TensorFlow Recommenders__: https://www.tensorflow.org/recommenders/examples/quickstart.\n",
    "\n",
    "Here we implement the basic retrieval model, which takes session data, pads it with zeros until the maximum session length, further transforms it into an emdedded representation of 32 values and fits it to the GRU layer. This way we receive a query representation, which is matched by dot product with a candidate representation, consisting of 32 values as well.  \n",
    "In this basic scenario for the __query model__ we take items in the session and for the __candidate model__ we take the purchsed item representation. \n",
    "\n",
    "The MRR for the basic model is 0.01. \n",
    "\n",
    "This model can be __improved__ by adding information about _timestamps_ and _item features_ to the query model and to the candidate model at the same time.  We can do it by normalizing timestamps and bucketing them. Item features can be also emdedded by concidering \"category_description\" as one categorical feature.\n",
    "\n",
    "Further the information from both models can by separately concatenated and fed into a dense layer to receive an equally-sized representations for a query and a candidate model. \n",
    "\n",
    "\n",
    "## 1 | Libaries & Data\n",
    "\n",
    "Import all libraries and the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f3e5d0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow_recommenders\n",
    "#!pip install tensorflow_ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "826fb087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import tempfile\n",
    "\n",
    "from typing import Dict, Text\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "import random\n",
    "import logging\n",
    "import pandas as pd\n",
    "import tensorflow_ranking as tfr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd5ab30",
   "metadata": {},
   "source": [
    "### Load sessions data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "499d3c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"~/shared/data/project/training/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b43411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_NAME = \"train_sessions.csv\"\n",
    "train_sessions = pd.read_csv(directory + DATA_NAME,parse_dates=['date'],dtype={\n",
    "                     'session_id': int,\n",
    "                     'item_id': int\n",
    "                 })\n",
    "train_sessions['timestemp'] = train_sessions['date'].values.astype('datetime64[s]').astype(np.int64) # to_unix/to_timestemp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aec4c1b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "session_id             int64\n",
       "item_id                int64\n",
       "date          datetime64[ns]\n",
       "timestemp              int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sessions.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c23e90a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop columns, group by session_id, and pad with 0 - length = max length\n",
    "train_sessions = train_sessions[['session_id', 'item_id','timestemp' ]]\n",
    "unique_item_ids = np.unique(train_sessions['item_id'])\n",
    "train_sessions = train_sessions[['session_id', 'item_id','timestemp' ]].groupby('session_id').agg(list).reset_index(level=0)\n",
    "\n",
    "# Pad with 0\n",
    "max_len = max([len(line) for line in train_sessions.item_id])\n",
    "[ line.extend([0]*(max_len-len(line))) for line in train_sessions.item_id if len(line)<max_len]\n",
    "[ line.extend([0]*(max_len-len(line))) for line in train_sessions.timestemp if len(line)<max_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bbe591",
   "metadata": {},
   "source": [
    "### Load Purchase data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07734c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_NAME = \"train_purchases.csv\"\n",
    "col_names=['session_id_p', 'item_id_p', 'date_p']\n",
    "train_purchases = pd.read_csv(directory+DATA_NAME,parse_dates=['date'],dtype={\n",
    "                     'session_id': int,\n",
    "                     'item_id': int\n",
    "                 })\n",
    "train_purchases['timestemp'] = train_purchases['date'].values.astype('datetime64[s]').astype(np.int64) # to_unix/to_timestemp\n",
    "unique_item_ids_p = np.unique(train_purchases['item_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aab25be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>timestemp</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>[9655, 9655, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1608326700, 1608326388, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>15085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>[15654, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[1584128127, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>18626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "      <td>[18316, 2507, 4026, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[1598469510, 1598469391, 1598469347, 0, 0, 0, ...</td>\n",
       "      <td>24911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19</td>\n",
       "      <td>[25772, 6341, 25555, 20033, 8281, 8268, 4385, ...</td>\n",
       "      <td>[1604334678, 1604334873, 1604335384, 160433532...</td>\n",
       "      <td>12534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>[2927, 11662, 2927, 28075, 434, 16064, 10414, ...</td>\n",
       "      <td>[1582737784, 1582737989, 1582737768, 158274135...</td>\n",
       "      <td>13226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   session_id                                            item_id  \\\n",
       "0           3  [9655, 9655, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1          13  [15654, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "2          18  [18316, 2507, 4026, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "3          19  [25772, 6341, 25555, 20033, 8281, 8268, 4385, ...   \n",
       "4          24  [2927, 11662, 2927, 28075, 434, 16064, 10414, ...   \n",
       "\n",
       "                                           timestemp  label  \n",
       "0  [1608326700, 1608326388, 0, 0, 0, 0, 0, 0, 0, ...  15085  \n",
       "1  [1584128127, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  18626  \n",
       "2  [1598469510, 1598469391, 1598469347, 0, 0, 0, ...  24911  \n",
       "3  [1604334678, 1604334873, 1604335384, 160433532...  12534  \n",
       "4  [1582737784, 1582737989, 1582737768, 158274135...  13226  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_sessions.merge(train_purchases, on='session_id', how='inner', suffixes=('_s', '_p'))\n",
    "train_df.drop('date',axis = 1, inplace  = True)\n",
    "train_df.rename(columns = { 'item_id_s':'item_id', 'timestemp_s': 'timestemp', 'item_id_p':'label'}, inplace = True)\n",
    "train_df.drop('timestemp_p',axis = 1, inplace  = True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "712d4b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>item_id_s</th>\n",
       "      <th>timestemp_s</th>\n",
       "      <th>item_id_p</th>\n",
       "      <th>timestemp_p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>[9655, 9655, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1608326700, 1608326388, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>15085</td>\n",
       "      <td>1608326807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>[15654, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[1584128127, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>18626</td>\n",
       "      <td>1584128175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "      <td>[18316, 2507, 4026, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[1598469510, 1598469391, 1598469347, 0, 0, 0, ...</td>\n",
       "      <td>24911</td>\n",
       "      <td>1598469632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19</td>\n",
       "      <td>[25772, 6341, 25555, 20033, 8281, 8268, 4385, ...</td>\n",
       "      <td>[1604334678, 1604334873, 1604335384, 160433532...</td>\n",
       "      <td>12534</td>\n",
       "      <td>1604337405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>[2927, 11662, 2927, 28075, 434, 16064, 10414, ...</td>\n",
       "      <td>[1582737784, 1582737989, 1582737768, 158274135...</td>\n",
       "      <td>13226</td>\n",
       "      <td>1582741664</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   session_id                                          item_id_s  \\\n",
       "0           3  [9655, 9655, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1          13  [15654, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "2          18  [18316, 2507, 4026, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "3          19  [25772, 6341, 25555, 20033, 8281, 8268, 4385, ...   \n",
       "4          24  [2927, 11662, 2927, 28075, 434, 16064, 10414, ...   \n",
       "\n",
       "                                         timestemp_s  item_id_p  timestemp_p  \n",
       "0  [1608326700, 1608326388, 0, 0, 0, 0, 0, 0, 0, ...      15085   1608326807  \n",
       "1  [1584128127, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      18626   1584128175  \n",
       "2  [1598469510, 1598469391, 1598469347, 0, 0, 0, ...      24911   1598469632  \n",
       "3  [1604334678, 1604334873, 1604335384, 160433532...      12534   1604337405  \n",
       "4  [1582737784, 1582737989, 1582737768, 158274135...      13226   1582741664  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12aebb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681129f9",
   "metadata": {},
   "source": [
    "## 2 | Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7120c42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000000it [01:33, 10679.61it/s]\n"
     ]
    }
   ],
   "source": [
    "examples = []\n",
    "for index,row in tqdm(train_df.iterrows()):\n",
    "    item_id = [x for x in row['item_id']]\n",
    "    timestemp = [x for x in row['timestemp']]\n",
    "    label = row['label']\n",
    "    feature = {\"context_item_id\":\n",
    "           tf.train.Feature(\n",
    "               int64_list=tf.train.Int64List(value= item_id)),\n",
    "           \"context_item_timestemp\":\n",
    "           tf.train.Feature(\n",
    "               int64_list=tf.train.Int64List(value= timestemp)),\n",
    "               \"label\":\n",
    "           tf.train.Feature(\n",
    "               int64_list=tf.train.Int64List(value= [label])),\n",
    "            }\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    examples.append(tf_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3799249",
   "metadata": {},
   "source": [
    "### Generate datasets and do the train|test split 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b535b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_TRAINING_DATA_FILENAME = 'train.tfrecord'\n",
    "OUTPUT_TESTING_DATA_FILENAME = 'test.tfrecord'\n",
    "\n",
    "def generate_train_test(examples, train_data_fraction=0.9,random_seed = 123, shuffle = True):\n",
    "    if shuffle:\n",
    "        random.seed(random_seed)\n",
    "        random.shuffle(examples)\n",
    "    last_train_index = round(len(examples) * train_data_fraction)\n",
    "    train_examples = examples[:last_train_index]\n",
    "    test_examples = examples[last_train_index:]\n",
    "    return train_examples, test_examples\n",
    "\n",
    "\n",
    "\n",
    "def write_tfrecords(tf_examples, filename):\n",
    "  \"\"\"Writes tf examples to tfrecord file, and returns the count.\"\"\"\n",
    "  with tf.io.TFRecordWriter(filename) as file_writer:\n",
    "    length = len(tf_examples)\n",
    "    progress_bar = tf.keras.utils.Progbar(length)\n",
    "    for example in tf_examples:\n",
    "      file_writer.write(example.SerializeToString())\n",
    "      progress_bar.add(1)\n",
    "    return length\n",
    "\n",
    "\n",
    "\n",
    "def generate_datasets(examples, \n",
    "                      random_seed = 123,\n",
    "                      shuffle = True,\n",
    "                      train_data_fraction=0.9,\n",
    "                      train_filename=OUTPUT_TRAINING_DATA_FILENAME,\n",
    "                      test_filename=OUTPUT_TESTING_DATA_FILENAME\n",
    "                     ):\n",
    "    \n",
    "    train_examples, test_examples = generate_train_test(examples,\n",
    "                                                        train_data_fraction=train_data_fraction,\n",
    "                                                        random_seed = random_seed,\n",
    "                                                        shuffle = shuffle)\n",
    "\n",
    "    logging.info(\"Writing generated training examples.\")\n",
    "    train_file = train_filename\n",
    "    train_size = write_tfrecords(tf_examples=train_examples, filename=train_file)\n",
    "    logging.info(\"Writing generated testing examples.\")\n",
    "    test_file = test_filename\n",
    "    test_size = write_tfrecords(tf_examples=test_examples, filename=test_file)\n",
    "    stats = {\n",
    "          \"train_size\": train_size,\n",
    "          \"test_size\": test_size,\n",
    "          \"train_file\": train_file,\n",
    "          \"test_file\": test_file,\n",
    "      }\n",
    "        \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "afe80a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900000/900000 [==============================] - 5s 6us/step\n",
      "100000/100000 [==============================] - 1s 6us/step\n"
     ]
    }
   ],
   "source": [
    "stats = generate_datasets(examples)\n",
    "logging.info(\"Generated dataset: %s\", stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b410bc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = \"train.tfrecord\"\n",
    "train = tf.data.TFRecordDataset(train_filename)\n",
    "\n",
    "test_filename = \"test.tfrecord\"\n",
    "test = tf.data.TFRecordDataset(test_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c46a2fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_description = {\n",
    "    'context_item_id': tf.io.FixedLenFeature([100], tf.int64, default_value=np.repeat(0, 100)),\n",
    "    'context_item_timestemp': tf.io.FixedLenFeature([100], tf.int64, default_value=np.repeat(0, 100)),\n",
    "    'label': tf.io.FixedLenFeature([1], tf.int64, default_value=0),\n",
    "}\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "    return tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "train_ds = train.map(_parse_function).map(lambda x: {\n",
    "    \"context_item_id\": tf.strings.as_string(x[\"context_item_id\"]),\n",
    "    #\"context_item_timestemp\": (x[\"context_item_timestemp\"]),\n",
    "    \"label\": tf.strings.as_string(x[\"label\"])\n",
    "})\n",
    "\n",
    "test_ds = test.map(_parse_function).map(lambda x: {\n",
    "    \"context_item_id\": tf.strings.as_string(x[\"context_item_id\"]),\n",
    "    #\"context_item_timestemp\": (x[\"context_item_timestemp\"]),\n",
    "    \"label\": tf.strings.as_string(x[\"label\"])\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bd514fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context_item_id': array([b'20754', b'10343', b'21564', b'21564', b'5890', b'0', b'0', b'0',\n",
      "       b'0', b'0', b'0', b'0', b'0', b'0', b'0', b'0', b'0', b'0', b'0',\n",
      "       b'0', b'0', b'0', b'0', b'0', b'0', b'0', b'0', b'0', b'0', b'0',\n",
      "       b'0', b'0', b'0', b'0', b'0', b'0', b'0', b'0', b'0', b'0', b'0',\n",
      "       b'0', b'0', b'0', b'0', b'0', b'0', b'0', b'0', b'0', b'0', b'0',\n",
      "       b'0', b'0', b'0', b'0', b'0', b'0', b'0', b'0', b'0', b'0', b'0',\n",
      "       b'0', b'0', b'0', b'0', b'0', b'0', b'0', b'0', b'0', b'0', b'0',\n",
      "       b'0', b'0', b'0', b'0', b'0', b'0', b'0', b'0', b'0', b'0', b'0',\n",
      "       b'0', b'0', b'0', b'0', b'0', b'0', b'0', b'0', b'0', b'0', b'0',\n",
      "       b'0', b'0', b'0', b'0'], dtype=object),\n",
      " 'label': array([b'24820'], dtype=object)}\n"
     ]
    }
   ],
   "source": [
    "for x in train_ds.take(1).as_numpy_iterator():\n",
    "  pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "08001fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find unique item_ids for the vocabulary\n",
    "unique_item_ids_p = np.unique(train_purchases['item_id']).tolist()\n",
    "unique_item_ids = unique_item_ids.tolist()\n",
    "unique_item_ids.extend(unique_item_ids_p)\n",
    "unique_item_ids = list(set(unique_item_ids))\n",
    "unique_item_ids = np.array(unique_item_ids).astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929b7ba9",
   "metadata": {},
   "source": [
    "## 3 | Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b1d73d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimension = 32\n",
    "\n",
    "query_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.StringLookup(\n",
    "      vocabulary=unique_item_ids, mask_token=None),\n",
    "    tf.keras.layers.Embedding(len(unique_item_ids) + 1, embedding_dimension), \n",
    "    tf.keras.layers.GRU(embedding_dimension)\n",
    "])\n",
    "\n",
    "candidate_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.StringLookup(\n",
    "      vocabulary=unique_item_ids, mask_token=None),\n",
    "  tf.keras.layers.Embedding(len(unique_item_ids) + 1, embedding_dimension)\n",
    "])\n",
    "\n",
    "\n",
    "task = tfrs.tasks.Retrieval(metrics=tfrs.metrics.FactorizedTopK(\n",
    "    candidates=item_ids.batch(128).map(candidate_model),\n",
    "    metrics=[\n",
    "        tfr.keras.metrics.MRRMetric(),\n",
    "        tf.keras.metrics.TopKCategoricalAccuracy()\n",
    "    ],\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a98d24c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_ids = tf.data.Dataset.from_tensor_slices(unique_item_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5255d6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tfrs.Model):\n",
    "\n",
    "    def __init__(self, query_model, candidate_model):\n",
    "        super().__init__()\n",
    "        self._query_model = query_model\n",
    "        self._candidate_model = candidate_model\n",
    "\n",
    "        self._task = task\n",
    "\n",
    "    def compute_loss(self, features, training=False):\n",
    "        watch_history = features[\"context_item_id\"]\n",
    "        watch_next_label = features[\"label\"]\n",
    "\n",
    "        query_embedding = self._query_model(watch_history)       \n",
    "        candidate_embedding = self._candidate_model(watch_next_label)\n",
    "\n",
    "        return self._task(query_embedding, candidate_embedding, compute_metrics = not training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67255b97",
   "metadata": {},
   "source": [
    "## 4 | Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dc87a1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fe8ec4b95e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fe8ec4b95e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fe8ec4b95e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "220/220 [==============================] - 104s 452ms/step - mrr_metric: 0.0000e+00 - top_k_categorical_accuracy: 0.0000e+00 - loss: 33976.6556 - regularization_loss: 0.0000e+00 - total_loss: 33976.6556\n",
      "Epoch 2/3\n",
      "220/220 [==============================] - 88s 400ms/step - mrr_metric: 0.0000e+00 - top_k_categorical_accuracy: 0.0000e+00 - loss: 33976.6435 - regularization_loss: 0.0000e+00 - total_loss: 33976.6435\n",
      "Epoch 3/3\n",
      "220/220 [==============================] - 90s 408ms/step - mrr_metric: 0.0000e+00 - top_k_categorical_accuracy: 0.0000e+00 - loss: 33976.6433 - regularization_loss: 0.0000e+00 - total_loss: 33976.6433\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe8ec428670>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(query_model, candidate_model)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))\n",
    "\n",
    "cached_train = train_ds.batch(4096).cache()\n",
    "cached_test = test_ds.batch(512).cache()\n",
    "\n",
    "model.fit(cached_train, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c43e88",
   "metadata": {},
   "source": [
    "## 5 | Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7bb8076f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fe8ec52ee50> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fe8ec52ee50> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fe8ec52ee50> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "196/196 [==============================] - 63s 313ms/step - mrr_metric: 0.0102 - top_k_categorical_accuracy: 3.2000e-04 - loss: 3169.8395 - regularization_loss: 0.0000e+00 - total_loss: 3169.8395\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mrr_metric': 0.01015275064855814,\n",
       " 'top_k_categorical_accuracy': 0.00031999999191612005,\n",
       " 'loss': 812.02783203125,\n",
       " 'regularization_loss': 0,\n",
       " 'total_loss': 812.02783203125}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(cached_test, return_dict=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
